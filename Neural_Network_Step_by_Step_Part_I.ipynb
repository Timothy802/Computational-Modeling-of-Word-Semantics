{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neual Network Step by Step - Part I\n",
    "---\n",
    "Draft by Timothy (tq.wang93@hotmail.com)\n",
    "\n",
    "### Logistic regression from scratch\n",
    "\n",
    "In part I, we will work on the simplest neural network, which includes an **_input layer_** with **_n_** nodes (each representing a **_feature_** of the input) and **_one_** node in the **_output layer_**. This type of neural network can be used to identify whether the item belongs to a certain category (Yes = 1; No = 0). \n",
    "\n",
    "For example, given a series of features ($ x_1, x_2, \\dots x_n $) of a picture, the model can judge whether it is a cat (Yes = 1; No = 0). \n",
    "\n",
    "<img style=\"float: center;\" src=\"Figures/Logistic_Regression_Cat_Example.png\" width=\"65%\">\n",
    "\n",
    "We will train this neural network using **_m_** samples. Before showing codes, let's go through the calculation of **_feed-forward_** and **_backward propagation_** process.\n",
    "\n",
    "<img style=\"float: center;\" src=\"Figures/Logistic_Regression.png\" width=\"65%\">\n",
    "\n",
    "#### Feed-forward\n",
    "\n",
    "- **For sample i**\n",
    "\n",
    "  Let's first define the vector representation of sample i and weight.\n",
    "  \n",
    "  $$\n",
    "   x^{(i)} = \\begin{pmatrix} x^i_1\\\\ x^i_2\\\\ \\vdots\\\\ x^i_n\\\\ \\end{pmatrix}_{n \\times 1} \\qquad\n",
    "   w = \\begin{pmatrix} w_1\\\\ w_2\\\\ \\vdots\\\\ w_n\\\\ \\end{pmatrix}_{n \\times 1} \n",
    "  $$\n",
    "  \n",
    "  We can then calculate z for sample i and represent it in a more concise way.\n",
    "\n",
    "  $$\n",
    "  \\begin{align*}\n",
    "   z^{(i)} &= x^i_1w_1 + x^i_1w_1 + \\dots + x^i_nw_n + b_i \\\\\n",
    "   &= (w_1,w_2,\\dots,w_n)\\begin{pmatrix}x^i_1\\\\ x^i_2\\\\ \\vdots\\\\ x^i_n\\\\ \\end{pmatrix} + b_i\\\\\n",
    "   &= w^Tx^{(i)} + b_i\n",
    "  \\end{align*}\n",
    "  $$\n",
    "  \n",
    "- **For all samples**\n",
    "\n",
    "  Now, we can define the vector representation of Z, w, X, and $\\hat{Y}$.\n",
    "\n",
    "  $$\n",
    "  \\begin{align*}\n",
    "   Z &= [z^{(1)},z^{(2)},\\dots,z^{(m)}]_{1 \\times m}\\\\\n",
    "   &= [w^Tx^{(1)}+b_1,w^Tx^{(2)}+b_2,\\dots,w^Tx^{(m)}+b_m]\\\\\n",
    "   &= w^T[x^{(1)},x^{(2)},\\dots,x^{(m)}]+[b_1,b_2,\\dots,b_m]\\\\\n",
    "   &= w^TX + b \\\\[10pt]\n",
    "   w^T &= [w_1,w_2,\\dots,w_n]_{1 \\times n} \\qquad\n",
    "   X = \\begin{pmatrix} \n",
    "      x^1_1 & x^2_1 & \\dots & x^m_1\\\\ \n",
    "      x^1_2 & x^2_2 & \\dots & x^m_2\\\\ \n",
    "      \\vdots& \\vdots& \\vdots& \\vdots\\\\ \n",
    "      x^1_n & x^2_n & \\dots & x^m_n\\\\ \\end{pmatrix}_{n \\times m}\\\\[10pt]\n",
    "  \\hat{Y} &= sigmoid(Z) = [\\hat{y}^{(1)},\\hat{y}^{(2)},\\dots,\\hat{y}^{(m)}]_{1 \\times m}\n",
    "  \\end{align*}\n",
    "  $$\n",
    "  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back propagation\n",
    "\n",
    "Back propagation starts by comparing the **_predicted value $ \\hat{y} $_** to the **_true value $ y $_**, which produces the **loss**. It then updates the weights $ w $ and bias $ b $ using **_gradient descent_**. Through iterations, the weights and bias are optimized.    \n",
    "<img style=\"float: center;\" src=\"Figures/Logistic_Regression_BP.png\" width=\"65%\">\n",
    "\n",
    "- **Loss function $L(a,y)$**\n",
    "\n",
    "  Here, we use the **_cross entropy_** loss function. There are other options such as **_mean squared error_**.\n",
    "  \n",
    "  $$L(a,y) = -ylog\\,a-(1-y)log(1-a)$$\n",
    "\n",
    "- **Cost function $J(a,y)$**\n",
    "\n",
    "  As defined, cost function is the **_mean_** of the loss of all samples.\n",
    "\n",
    "  $$J(a,y) = J(w,b)=\\frac{1}{m} \\sum_{i=1}^{m}L(a,y)$$\n",
    "\n",
    "- **Gradient descent**\n",
    "\n",
    "  Based on **_chain rules_**,\n",
    "  \n",
    "  $$\n",
    "   \\frac{\\partial L(a,y)}{\\partial w} = \n",
    "   \\frac{\\partial L(a,y)}{\\partial a}\\cdot\n",
    "   \\frac{\\partial a}{\\partial z}\\cdot\n",
    "   \\frac{\\partial z}{\\partial w}\\\\[10pt]\n",
    "  $$\n",
    "  $$\n",
    "   \\frac{\\partial L(a,y)}{\\partial b} = \n",
    "   \\frac{\\partial L(a,y)}{\\partial a}\\cdot\n",
    "   \\frac{\\partial a}{\\partial z}\\cdot\n",
    "   \\frac{\\partial z}{\\partial b}\n",
    "  $$\n",
    "  \n",
    "  **Note:** the *sigmoid* function is\n",
    "  $$a = \\sigma(z) = \\frac{1}{1+e^{-z}} $$\n",
    "\n",
    "  $$\n",
    "  \\begin{align*}\n",
    "  \\frac{\\partial L(a,y)}{\\partial a} \n",
    "   &= -y \\, \\frac{1}{a} - (1-y) \\, \\frac{1}{1-a}(-1)\\\\\n",
    "   &= -\\frac{y}{a} + \\frac{1-y}{1-a}\n",
    "  \\end{align*}\n",
    "  $$\n",
    "  \n",
    "  $$\n",
    "  \\begin{align*}\n",
    "  \\frac{\\partial a}{\\partial z} \n",
    "   &= \\frac{0-1 \\cdot e^{-z} \\cdot (-1)}{(1 + e^{-z})^2}\\\\\n",
    "   &= \\frac{e^{-z}}{(1 + e^{-z})^2}\\\\\n",
    "   &= \\frac{1}{1 + e^{-z}} \\cdot \\frac{e^{-z}}{1 + e^{-z}}\\\\\n",
    "   &= \\frac{1}{1 + e^{-z}} \\cdot (1-\\frac{1}{1 + e^{-z}})\\\\\n",
    "   &= a \\cdot (1-a)\n",
    "  \\end{align*}\n",
    "  $$\n",
    "\n",
    "  Therefore,\n",
    "  $$\n",
    "  \\begin{align*}\n",
    "  \\frac{\\partial L(a,y)}{\\partial a} \\cdot\n",
    "  \\frac{\\partial a}{\\partial z}\n",
    "   &= (-\\frac{y}{a} + \\frac{1-y}{1-a}) \\, a(1-a)\\\\\n",
    "   &= -y(1-a)+a(1-y)\\\\\n",
    "   &= a-y\n",
    "  \\end{align*}\n",
    "  $$\n",
    "\n",
    "  Moving on, for the partial derivatives of $ z = w^T + b $  to $ w $ and $ b $,\n",
    "\n",
    "  1. In the case of **sample i**, because\n",
    "    $$\n",
    "    \\begin{gather*}\n",
    "    \\frac{\\partial z}{\\partial w} = x_{n \\times 1}\\qquad\n",
    "    \\frac{\\partial x}{\\partial b} = 1_{1 \\times 1}\n",
    "    \\end{gather*}\n",
    "    $$\n",
    "    \n",
    "    we can get\n",
    "    \n",
    "    $$\\frac{\\partial L(a,y)}{\\partial w} = x_{n \\times 1}(a-y)_{1 \\times 1}$$\n",
    "    $$\\frac{\\partial L(a,y)}{\\partial b} = (a-y)_{1 \\times 1} \\cdot 1_{1 \\times 1}$$\n",
    "\n",
    "  2. In the case of **m samples**,\n",
    "\n",
    "    $$\\frac{\\partial L(A,Y)}{\\partial w_{n \\times 1}} = X_{n \\times m}(A-Y)^T \\, _{m \\times 1}$$\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    \\frac{\\partial L(A,Y)}{\\partial b_{1 \\times 1}}\n",
    "     &= A-Y \\, _{1 \\times m}\\\\\n",
    "     &= np.sum(A-Y) \\, _{1 \\times 1}\n",
    "    \\end{align*}$$\n",
    "\n",
    "* **Updating weights and bias**\n",
    "\n",
    "  The weight $ w $ and bias $ b $ are updated by subtracting the **_mean of cost_** multiplied by the **_learning rate $ \\alpha $_**.\n",
    "  $$\\begin{alignat}{0}\n",
    "   w \\, \\gets \\, w - \\alpha \\cdot \\frac{1}{m} \\cdot X(A-Y)^T  \n",
    "  \\\\[10pt]\n",
    "   b \\, \\gets \\, b - \\alpha \\cdot \\frac{1}{m} \\cdot np.sum(A-Y)\n",
    "  \\end{alignat}$$\n",
    "\n",
    "* **Revisiting the cost function J**\n",
    "\n",
    "  Usually, we print/plot the change of cost as a function of iterations. \n",
    "  $$\\begin{align}    \n",
    "    J &= \\frac{1}{m} \\sum_{i=1}^{m} [-ylog\\,a-(1-y)log(1-a)]\\\\\n",
    "     &= -\\frac{1}{m} \\sum_{i=1}^{m} [ylog\\,a+(1-y)log(1-a)]\\\\\n",
    "     &= -\\frac{1}{m} [Ylog\\,A+(1-Y)log(1-A)]\\\\\n",
    "     &= -\\frac{1}{m} np.sum[Ylog\\,A+(1-Y)log(1-A)]\n",
    "  \\end{align}$$\n",
    "  **Note:** the $ Ylog\\,A+(1-Y)log(1-A) $ is calculated using dot product. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding a logistic regression\n",
    "\n",
    "#### Code: sigmoid function\n",
    "\n",
    "$$\n",
    "a = \\frac{1}{1+e^{-z}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sigmoid function\n",
    "def sigmoid(z):\n",
    "    a = 1 / (1 + np.exp(-z))\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code: feed-forward, cost function and gradient descent\n",
    "\n",
    "- **Feed-forward**\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Z &= w^TX + b \\\\[10pt]\n",
    "A &= \\sigma(Z)\\\\[10pt]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- **Cost function**\n",
    "$$\n",
    "J = -\\frac{1}{m} np.sum[Ylog\\,A+(1-Y)log(1-A)]\n",
    "$$\n",
    "\n",
    "- **Gradient descent**\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L(A,Y)}{\\partial w} &= X (A-Y)^T\n",
    "\\\\[12pt]\n",
    "\\frac{\\partial L(A,Y)}{\\partial b} &= np.sum(A-Y)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "n_dim = train_data_sta.shape[0] # number of rows in training data\n",
    "w = np.zeros((n_dim, 1))\n",
    "b = 0\n",
    "\n",
    "# propagate\n",
    "def propagate(w, b, X, Y):\n",
    "    \n",
    "    # feed-forward function\n",
    "    Z = np.dot(w.T, X) + b # np.dot -> matrix multiplication\n",
    "    A = sigmoid(Z)\n",
    "    \n",
    "    # cost function\n",
    "    m = X.shape[1]\n",
    "    J = -1/m * np.sum(Y * np.log(A) + (1-Y) * np.log(1-A))\n",
    "    \n",
    "    # gradient descent (Note: mean)\n",
    "    dw = 1/m * np.dot(X,(A-Y).T)\n",
    "    db = 1/m * np.sum(A-Y)\n",
    "    \n",
    "    grands = {'dw': dw, 'db': db}\n",
    "    \n",
    "    return grands, J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code: optimization\n",
    "\n",
    "$$\n",
    "\\begin{alignat}{0}\n",
    "w \\, \\gets \\, w - \\alpha \\cdot \\frac{1}{m} \\cdot X(A-Y)^T \\\\[10pt]\n",
    "b \\, \\gets \\, b - \\alpha \\cdot \\frac{1}{m} \\cdot np.sum(A-Y)\n",
    "\\end{alignat}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization\n",
    "def optimize(w, b, X, Y, alpha, n_iters):\n",
    "    costs = []\n",
    "    for i in range(n_iters):\n",
    "        grands, J = propagate(w, b, X, Y)\n",
    "        dw = grands['dw']\n",
    "        db = grands['db']\n",
    "        \n",
    "        w = w - alpha * dw\n",
    "        b = b - alpha * db\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            costs.append(J)\n",
    "            print('Epoch %d: cost = %.4f' % (i+1, J))\n",
    "     \n",
    "    grands = {'dw': dw, 'db': db}\n",
    "    params = {'w': w, 'b': b}\n",
    "    \n",
    "    return grands, params, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code: prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "def predict(w, b, X_test):\n",
    "    \n",
    "    Z = np.dot(w.T, X_test) + b\n",
    "    A = sigmoid(Z)\n",
    "    \n",
    "    m = X_test.shape[1]\n",
    "    Y_pred = np.zeros((1, m))\n",
    "    \n",
    "    for i in range(m):\n",
    "        if A[:, i] > 0.5:\n",
    "            Y_pred[:, i] = 1\n",
    "        else:\n",
    "            Y_pred[:, i] = 0\n",
    "    \n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code: integrating previous steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrating previous steps\n",
    "def model(w, b, X_train, X_test, Y_train, Y_test, alpha, n_iters):\n",
    "    grands, params, costs = optimize(w, b, X_train, Y_train, alpha, n_iters)\n",
    "    w = params['w']\n",
    "    b = params['b']\n",
    "    \n",
    "    Y_pred_train = predict(w, b, X_train)\n",
    "    Y_pred_test = predict(w, b, X_test)\n",
    "    \n",
    "    print('Train accuracy: %.2f' % np.mean(y_pred_train == y_train))\n",
    "    print('Test accuracy: %.2f' % np.mean(y_pred_test == y_test))\n",
    "    \n",
    "    dic = {\n",
    "           'w': w,\n",
    "           'b': b,\n",
    "           'costs': costs,\n",
    "           'y_pred_train': y_pred_train,\n",
    "           'y_pred_test': y_pred_test,\n",
    "           'alpha': alpha\n",
    "    }\n",
    "    \n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code: train and test\n",
    "\n",
    "- Train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = model(w, b, \n",
    "            train_data_sta, train_labels_tran, \n",
    "            test_data_sta, test_labels_tran,\n",
    "            alpha = 0.005, n_iters = 2000\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot the change of cost as a function of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(b['costs'])\n",
    "plt.xlabel('per hundred iterations')\n",
    "plt.ylabel('cost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code: predict a picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "print('True label: %d' % test_labels_tran[0, index])\n",
    "print('Pred label: %d' % int(b['y_pred_test'][0, index]))\n",
    "\n",
    "# show the picture\n",
    "plt.imshow(test_data_org[index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
